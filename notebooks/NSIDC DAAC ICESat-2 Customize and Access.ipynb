{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSIDC DAAC Customize and Access Data Tutorial\n",
    "### This tutorial will walk you though how to access NSIDC DAAC data using spatial and temporal filters, as well as how to request customization services including subsetting, reformatting, and reprojection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, getpass, socket, json, zipfile, io, math, os, shutil, pprint\n",
    "from statistics import mean\n",
    "from requests.auth import HTTPBasicAuth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a token\n",
    "\n",
    "### We will generate a token needed in order to access data using your Earthdata Login credentials, and we will apply that token to the following queries. If you do not already have an Earthdata Login account, go to http://urs.earthdata.nasa.gov to register. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earthdata Login credentials\n",
    "\n",
    "uid = input('Earthdata Login user name: ')\n",
    "pswd = getpass.getpass('Earthdata Login password: ')\n",
    "email = input('Email address associated with Earthdata Login account: ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "# Request token from Common Metadata Repository using Earthdata credentials\n",
    "token_api_url = 'https://api.echo.nasa.gov/echo-rest/tokens'\n",
    "hostname = socket.gethostname()\n",
    "ip = socket.gethostbyname(hostname)\n",
    "\n",
    "data = {\n",
    "    'token': {\n",
    "        'username': uid,\n",
    "        'password': pswd,\n",
    "        'client_id': 'NSIDC_client_id',\n",
    "        'user_ip_address': ip\n",
    "    }\n",
    "}\n",
    "headers={'Accept': 'application/json'}\n",
    "response = requests.post(token_api_url, json=data, headers=headers)\n",
    "token = json.loads(response.content)['token']['id']\n",
    "#response = requests.post(token_api_url, data=json, headers=headers)\n",
    "#token = requests.post('https://api.echo.nasa.gov/echo-rest/tokens', data=xml, headers=headers)\n",
    "print(token)\n",
    "\n",
    "# Back-up token in case of CMR downtime:\n",
    "\n",
    "# token = '3035A8A7-A7EE-E3B8-5B9C-64E81E1C8EAC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a data set of interest and determine the number and size of granules available within a time range and location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's begin discovering an NSIDC DAAC data set by first inputting the data set of interest and determining the most recent version number. We will also find out how many data granules exist over an area and time of interest. The Common Metadata Repository is queried to explore this information.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data set short name (e.g. ATL03) of interest here.\n",
    "\n",
    "short_name = input('Input short name, e.g. ATL03, here: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get json response from CMR collection metadata\n",
    "\n",
    "params = {\n",
    "    'short_name': short_name\n",
    "}\n",
    "\n",
    "cmr_collections_url = 'https://cmr.earthdata.nasa.gov/search/collections.json'\n",
    "response = requests.get(cmr_collections_url, params=params)\n",
    "results = json.loads(response.content)\n",
    "\n",
    "# Find all instances of 'version_id' in metadata and print most recent version number\n",
    "\n",
    "versions = [el['version_id'] for el in results['feed']['entry']]\n",
    "latest_version = max(versions)\n",
    "print('The most recent version of ', short_name, ' is ', latest_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have the most recent version of this data set, let's determine the number of granules available over our area and time of interest.\n",
    "\n",
    "### Let's first choose how we want to input our area of interest, either as spatial bounding box, polygon coordinate pairs, or via shapefile or KML upload. *Note that only ICESat-2 data can be subsetted by polygon in addition to bounding box at this time, so if you wish to subset (which we will get to in more detail below), note that you will need to modify your area of interest to a bounding box for all other data sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi = str(input('The following Area of Interest selections are available: As 1) Bounding Box, 2) Polygon coordinate pairs, or 3) Spatial file input, including Esri Shapefile, KML/KMZ, and GeoJSON. Enter your selected numbered option (1, 2, or 3).'))\n",
    "\n",
    "if aoi == '1': \n",
    "    #Input bounding box\n",
    "    LL_lon = input('Input lower left longitude in decimal degrees: ')\n",
    "    LL_lat = input('Input lower left latitude in decimal degrees: ')\n",
    "    UR_lon = input('Input upper right longitude in decimal degrees: ')\n",
    "    UR_lat = input('Input upper right latitude in decimal degrees: ')\n",
    "    \n",
    "    bounding_box = LL_lon + ',' + LL_lat + ',' + UR_lon + ',' + UR_lat\n",
    "    \n",
    "elif aoi == '2':\n",
    "    print('Polygon points need to be provided in counter-clockwise order. The last point should match the first point to close the polygon.')\n",
    "    polygon = input('Input polygon coordinates as comma separated values in longitude latitude order, i.e. lon1, lat1, lon2, lat2, lon3, lat3, and so on: ')\n",
    "    \n",
    "elif aoi == '3':\n",
    "    \n",
    "# You can perform Ogre transformations directly by making a HTTP POST request:\n",
    "\n",
    "# Convert to GeoJSON\n",
    "\n",
    "# http://ogre.adc4gis.com/convert with the following params:\n",
    "# upload - the file being uploaded\n",
    "# sourceSrs (optional) - the original projection\n",
    "# targetSrs (optional) - the target projection\n",
    "# forcePlainText (optional) - force `text/plain` instead of `application/json`\n",
    "# skipFailures (optional) - skip failures\n",
    "# callback (optional) - a JSONP callback function name\n",
    "\n",
    "\n",
    "#response = requests.post(token_api_url, json=data, headers=headers)\n",
    "url = 'http://ogre.adc4gis.com/convert'\n",
    "shapefile = '/Users/asteiker/Desktop/Data_services/polygon-testing/USA.kml'\n",
    "files = {'file': open(shapefile, 'rb')}\n",
    "\n",
    "r = requests.post(url, files=files)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response = requests.post(token_api_url, json=data, headers=headers)\n",
    "url = 'http://ogre.adc4gis.com/convert'\n",
    "shapefile = '/Users/asteiker/Desktop/Data_services/polygon-testing/USA.kml'\n",
    "files = {'file': open(shapefile, 'rb')}\n",
    "print(files)\n",
    "r = requests.post(url, files=files)\n",
    "#print(r)\n",
    "#r.text\n",
    "results = json.loads(r.content)\n",
    "print(results)\n",
    "#r.headers\n",
    "\n",
    "#r = requests.post(url, files=files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input temporal range \n",
    "\n",
    "start_date = input('Input start date in yyyy-MM-dd format: ')\n",
    "start_time = input('Input start time in HH:mm:ss format: ')\n",
    "end_date = input('Input end date in yyyy-MM-dd format: ')\n",
    "end_time = input('Input end time in HH:mm:ss format: ')\n",
    "\n",
    "temporal = start_date + 'T' + start_time + 'Z' + ',' + end_date + 'T' + end_time + 'Z'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query number of granules (paging over results)\n",
    "\n",
    "granule_search_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n",
    "params = {\n",
    "    'short_name': short_name,\n",
    "    #'version': latest_version,\n",
    "    'version': 209\n",
    "    'bounding_box': bounding_box,\n",
    "    'temporal': temporal,\n",
    "    'page_size': 100,\n",
    "    'page_num': 1\n",
    "}\n",
    "granules = []\n",
    "while True:\n",
    "    response = requests.get(granule_search_url, params=params, headers=headers)\n",
    "    results = json.loads(response.content)\n",
    "\n",
    "    if len(results['feed']['entry']) == 0:\n",
    "        # Out of results, so break out of loop\n",
    "        break\n",
    "\n",
    "    # Collect results and increment page_num\n",
    "    granules.extend(results['feed']['entry'])\n",
    "    params['page_num'] += 1\n",
    "\n",
    "print('There are', len(granules), 'granules of', short_name, 'version', latest_version, 'over my area and time of interest.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now query the average size of those granules as well as the total volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granule_sizes = [float(granule['granule_size']) for granule in granules]\n",
    "\n",
    "print(f'The average size of each granule is {mean(granule_sizes):.2f} MB and the total size of all {len(granules)} granules is {sum(granule_sizes):.2f} MB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Although subsetting, reformatting, or reprojecting can alter the size of the granules, this \"native\" granule size can still be used to guide us towards the best download method to pursue, which we will come back to later on in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the subsetting, reformatting, and reprojection services enabled for your data set of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NSIDC DAAC supports customization services on many of our NASA Earthdata mission collections. Let's discover whether or not our data set has these services available. If services are available, we will also determine the specific service options supported for this data set and select which of these services we want to request. \n",
    "\n",
    "### We will start by querying the service capability endpoint and gather service information that we will select in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query service capability URL \n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "capability_url = f'https://n5eil02u.ecs.nsidc.org/egi/capabilities/{short_name}.{latest_version}.xml'\n",
    "#capability_url = 'https://n5eil02u.ecs.nsidc.org/egi/capabilities/SPL3SMP.005.xml'\n",
    "\n",
    "# Create session to store cookie and pass credentials to capabilities url\n",
    "\n",
    "session = requests.session()\n",
    "s = session.get(capability_url)\n",
    "response = session.get(s.url,auth=(uid,pswd))\n",
    "\n",
    "root = ET.fromstring(response.content)\n",
    "\n",
    "#collect lists with each service option\n",
    "\n",
    "subagent = [subset_agent.attrib for subset_agent in root.iter('SubsetAgent')]\n",
    "\n",
    "# variable subsetting\n",
    "variables = [SubsetVariable.attrib for SubsetVariable in root.iter('SubsetVariable')]  \n",
    "variables_raw = [variables[i]['value'] for i in range(len(variables))]\n",
    "variables_join = [''.join(('/',v)) if v.startswith('/') == False else v for v in variables_raw] \n",
    "variable_vals = [v.replace(':', '/') for v in variables_join]\n",
    "\n",
    "# reformatting\n",
    "formats = [Format.attrib for Format in root.iter('Format')]\n",
    "format_vals = [formats[i]['value'] for i in range(len(formats))]\n",
    "format_vals.remove('')\n",
    "\n",
    "# reformatting options that support reprojection\n",
    "normalproj = [Projections.attrib for Projections in root.iter('Projections')]\n",
    "normalproj_vals = []\n",
    "normalproj_vals.append(normalproj[0]['normalProj'])\n",
    "format_proj = normalproj_vals[0].split(',')\n",
    "format_proj.remove('')\n",
    "format_proj.append('No reformatting')\n",
    "\n",
    "# reprojection options\n",
    "projections = [Projection.attrib for Projection in root.iter('Projection')]\n",
    "proj_vals = []\n",
    "for i in range(len(projections)):\n",
    "    if (projections[i]['value']) != 'NO_CHANGE' :\n",
    "        proj_vals.append(projections[i]['value'])\n",
    "        \n",
    "# reformatting options that do not support reprojection\n",
    "no_proj = [i for i in format_vals if i not in format_proj]\n",
    "\n",
    "# SMAP-specific reprojection logic\n",
    "\n",
    "#L1-L2 reprojection/reformatting options\n",
    "if short_name == 'SPL1CTB' or 'SPL1CTB_E' or 'SPL2SMA' or 'SPL2SMP' or 'SPL2SMP_E' or 'SPL2SMAP': \n",
    "    format_proj = ['GeoTIFF', 'NetCDF4-CF', 'HDF-EOS5']\n",
    "    no_proj = [i for i in format_vals if i not in format_proj]\n",
    "elif short_name == 'SPL2SMAP_S' or 'SPL3SMA' or 'SPL3SMP' or 'SPL3SMP_E' or 'SPL3SMAP' or 'SPL3FTA' or 'SPL3FTP' or 'SPL3FTP_E' or 'SPL4SMAU' or 'SPL4SMGP' or 'SPL4SMLM' or 'SPL4CMDL': \n",
    "    format_proj = ['No reformatting', 'GeoTIFF', 'NetCDF4-CF', 'HDF-EOS5']\n",
    "    no_proj = [i for i in format_vals if i not in format_proj]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now select subsetting, reformatting, and reprojection service options, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print service information depending on service availability and select service options\n",
    "    \n",
    "if len(subagent) < 1 :\n",
    "    print('No services exist for', short_name, 'version', latest_version)\n",
    "    agent = 'NO'\n",
    "    bbox = ''\n",
    "    time = ''\n",
    "    reformat = ''\n",
    "    projection = ''\n",
    "    projection_parameters = ''\n",
    "    coverage = ''\n",
    "else:\n",
    "    agent = ''\n",
    "    subdict = subagent[0]\n",
    "    if subdict['spatialSubsetting'] == 'true':\n",
    "        ss = input('Subsetting by bounding box, based on the area of interest inputted above, is available. Would you like to request this service? (y/n)')\n",
    "        if ss == 'y': bbox = bounding_box\n",
    "        else: bbox = ''\n",
    "    if subdict['temporalSubsetting'] == 'true':\n",
    "        ts = input('Subsetting by time, based on the temporal range inputted above, is available. Would you like to request this service? (y/n)')\n",
    "        if ts == 'y': time = temporal \n",
    "        else: time = ''\n",
    "    else: time = ''\n",
    "    if len(format_vals) > 0 :\n",
    "        print('These reformatting options are available:', format_vals)\n",
    "        reformat = input('If you would like to reformat, copy and paste the reformatting option you would like (make sure to omit quotes, e.g. GeoTIFF), otherwise leave blank.')\n",
    "        # select reprojection options based on reformatting selection\n",
    "        if reformat in format_proj and len(proj_vals) > 0 : \n",
    "            print('These reprojection options are available with your requested format:', proj_vals)\n",
    "            projection = input('If you would like to reproject, copy and paste the reprojection option you would like (make sure to omit quotes, e.g. GEOGRAPHIC), otherwise leave blank.')\n",
    "            # Enter required parameters for UTM North and South\n",
    "            if projection == 'UTM NORTHERN HEMISPHERE' or projection == 'UTM SOUTHERN HEMISPHERE': \n",
    "                NZone = input('Please enter a UTM zone (1 to 60 for Northern Hemisphere; -60 to -1 for Southern Hemisphere):')\n",
    "                projection_parameters = str('NZone:' + NZone)\n",
    "            else: projection_parameters = ''\n",
    "        else: \n",
    "            print('No reprojection options are supported with your requested format')\n",
    "            projection = ''\n",
    "            projection_parameters = ''\n",
    "    else: \n",
    "        reformat = ''\n",
    "        projection = ''\n",
    "        projection_parameters = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because variable subsetting can include a long list of variables to choose from, we will decide on variable subsetting separately from the service options above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variable subsetting\n",
    "\n",
    "if len(variable_vals) > 0:\n",
    "        v = input('Variable subsetting is available. Would you like to subset a selection of variables? (y/n)')\n",
    "        if v == 'y':\n",
    "            print('The', short_name, 'variables to select from include:')\n",
    "            pprint.pprint(variable_vals)\n",
    "            coverage = input('If you would like to subset by variable, copy and paste the variables you would like separated by comma. Make sure to omit quotes but include all forward slashes: ')\n",
    "        else: coverage = ''\n",
    "\n",
    "#no services selected\n",
    "if reformat == '' and projection == '' and projection_parameters == '' and coverage == '' and time == '' and bbox == '':\n",
    "    agent = 'NO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request data from the NSIDC data access service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now set up our data download request. Recall that we queried the total number and volume of granules prior to applying customization services, so you can use these values to adjust the number of granules per request up to a limit of 100 granules. For now, let's select 10 granules to be processed in each zipped request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many individual orders we will request based on the number of granules requested\n",
    "\n",
    "# Set number of granules requested per order, which we will initially set to 10.\n",
    "page_size = 10\n",
    "page_num = math.ceil(len(granules)/page_size)\n",
    "\n",
    "#Set NSIDC data access base URL\n",
    "base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request'\n",
    "\n",
    "print('There will be', page_num, 'total order(s) processed for our', short_name, 'request.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's download the data directly to this notebook directory in a new Outputs folder. The progress of each order will be reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Outputs folder if folder does not already exist, request data service for each page number, and unzip outputs\n",
    "\n",
    "path = str(os.getcwd() + '/Outputs')\n",
    "\n",
    "if os.path.exists(path):\n",
    "    for i in range(page_num):\n",
    "        page_val = i + 1\n",
    "        request_params = {'short_name': short_name, 'version': latest_version, 'temporal': temporal, 'time': time, 'bounding_box': bounding_box, 'bbox': bbox, 'format': reformat, 'projection': projection, 'projection_parameters': projection_parameters, 'Coverage': coverage, 'page_size': page_size, 'page_num': page_val, 'agent': agent, 'token': token, 'email': email, }\n",
    "        print('Data request', page_val, 'is processing...')\n",
    "        r = requests.get(base_url, params=request_params)\n",
    "        with zipfile.ZipFile(io.BytesIO(r.content)) as z:\n",
    "            z.extractall(path)\n",
    "        print('Data request', page_val, 'is complete.')\n",
    "else:\n",
    "    path = str(os.getcwd() + '/Outputs')\n",
    "    os.mkdir(path)\n",
    "    for i in range(page_num):\n",
    "        page_val = i + 1\n",
    "        request_params = {'short_name': short_name, 'version': latest_version, 'temporal': temporal, 'time': time, 'bounding_box': bounding_box, 'bbox': bbox, 'format': reformat, 'projection': projection, 'projection_parameters': projection_parameters, 'Coverage': coverage, 'page_size': page_size, 'page_num': page_val, 'agent': agent, 'token': token, 'email': email, }\n",
    "        print('Data request', page_val, 'is processing...')\n",
    "        r = requests.get(base_url, params=request_params)\n",
    "        with zipfile.ZipFile(io.BytesIO(r.content)) as z:\n",
    "            z.extractall(path)\n",
    "        print('Data request', page_val, 'is complete.')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we will clean up the Output folder by removing individual order folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up Outputs folder by removing individual granule folders \n",
    "\n",
    "for root, dirs, files in os.walk(path, topdown=False):\n",
    "    for file in files:\n",
    "        try:\n",
    "            shutil.move(os.path.join(root, file), path)\n",
    "        except OSError:\n",
    "            pass\n",
    "        \n",
    "for root, dirs, files in os.walk(path):\n",
    "    for name in dirs:\n",
    "        os.rmdir(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To review, we have explored data availability and volume over a region and time of interest, discovered and selected data customization options, and downloaded data directly to our local machine. You are welcome to request different data sets, areas of interest, and/or customization services by re-running the notebook or starting again at the 'Select a data set of interest' step above. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
